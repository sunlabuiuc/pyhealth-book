{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f09bf6",
   "metadata": {},
   "source": [
    "# HW4 CAML\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this question, we will implement Convolutional Attention for Multi-Label classification (CAML) proposed by Mullenbach et al. in the paper \"[Explainable Prediction of Medical Codes from Clinical Text](https://www.aclweb.org/anthology/N18-1100/)\".\n",
    "\n",
    "Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. Thus, let us implement CAML, an attentional convolutional network to predict medical codes from clinical text.\n",
    "\n",
    "<img src='img/clinical notes.png'>\n",
    "\n",
    "Image courtsey: [link](https://www.aclweb.org/anthology/2020.acl-demos.33/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5958c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfbaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# set up your own data path !!!\n",
    "DATA_PATH = \"IU-chest-Xray-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6e513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cb7522",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this question, we will be using the Indiana University Chest X-Ray dataset. The goal is to predict diseases using chest x-ray reports.\n",
    "\n",
    "Navigate to the data folder `DATA_PATH`, there are several files:\n",
    "\n",
    "- `train_df.csv`, `test_df.csv`: these two files contains the data used for training and testing.\n",
    "    - `Report ID` refers to a unique chest x-ray report.\n",
    "    - `Text` refers to the clinical report text.\n",
    "    - `Label` refers to the diseases.\n",
    "- `vocab.csv`: this file contains the vocabularies used in the clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6684e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.csv  train_df.csv  vocab.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a436af",
   "metadata": {},
   "source": [
    "For example, the first chest x-ray report in `train_df.csv` has:\n",
    "- `Report ID`: 1\n",
    "- `Text`: the cardiac silhouette and mediastinum size are within normal limits . there is no pulmonary edema . there is no focal consolidation . there are no xxxx of a pleural effusion . there is no evidence of pneumothorax . normal chest xxxxx .\n",
    "- `Label`: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "where label is a multi-hot vector representing the following diseases:\n",
    "```\n",
    "normal\n",
    "cardiomegaly\n",
    "scoliosis / degenerative\n",
    "fractures bone\n",
    "pleural effusion\n",
    "thickening\n",
    "pneumothorax\n",
    "hernia hiatal\n",
    "calcinosis\n",
    "emphysema / pulmonary emphysema\n",
    "pneumonia / infiltrate / consolidation\n",
    "pulmonary edema\n",
    "pulmonary atelectasis\n",
    "cicatrix\n",
    "opacity\n",
    "nodule / mass\n",
    "airspace disease\n",
    "hypoinflation / hyperdistention\n",
    "catheters indwelling / surgical instruments / tube inserted / medical device\n",
    "other\n",
    "```\n",
    "\n",
    "So this report 1 is labeled as \"normal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224160da",
   "metadata": {},
   "source": [
    "## 1 Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4348d43",
   "metadata": {},
   "source": [
    "### 1.1 Helper Functions\n",
    "\n",
    "To begin, weith, let us first implement some helper functions we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07502a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(sequence, token2idx):\n",
    "    \"\"\"\n",
    "    convert the sequnce of tokens to indices. \n",
    "    If the word in unknown, then map it to '<unk>'.\n",
    "    \n",
    "    INPUT:\n",
    "        sequence (type: list of str): a sequence of tokens\n",
    "        token2idx (type: dict): a dictionary mapping token to the corresponding index\n",
    "    \n",
    "    OUTPUT:\n",
    "        indices (type: list of int): a sequence of indicies\n",
    "        \n",
    "    EXAMPLE:\n",
    "        >>> sequence = ['hello', 'world', 'unknown_word']\n",
    "        >>> token2idx = {'hello': 0, 'world': 1, '<unk>': 2}\n",
    "        >>> to_index(sequence, token2idx)\n",
    "        [0, 1, 2]\n",
    "    \"\"\"\n",
    "    return [token2idx[w] if w in token2idx else token2idx['<unk>'] for w in sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3d285",
   "metadata": {},
   "source": [
    "### 1.2 CustomDataset [10 points]\n",
    "\n",
    "Now, let us implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the clinical text as input and medical codes as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed9f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "NUM_WORDS = 1253\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filename):        \n",
    "        # read in the data files\n",
    "        self.data = pd.read_csv(filename)\n",
    "        # load word lookup\n",
    "        self.idx2word, self.word2idx = self.load_lookup(f'{DATA_PATH}/vocab.csv', padding=True)\n",
    "        assert len(self.idx2word) == len(self.word2idx) == NUM_WORDS\n",
    "        \n",
    "    def load_lookup(self, filename, padding=False):\n",
    "        \"\"\" load lookup for word \"\"\"\n",
    "        idx2token = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                idx2token[i] = line\n",
    "        token2idx = {w:i for i,w in idx2token.items()}\n",
    "        return idx2token, token2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. admissions).\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        return len(self.data)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generate one sample of data.\n",
    "\n",
    "        Hint: convert text to indices using to_index();\n",
    "        \"\"\"\n",
    "        data = self.data.iloc[index]\n",
    "        text = data['Text'].split(' ')\n",
    "        label = data['Label']\n",
    "        # convert label string to list\n",
    "        label = [int(l) for l in label.strip('[]').split(', ')]\n",
    "        assert len(label) == NUM_CLASSES\n",
    "        ### BEGIN SOLUTION\n",
    "        text = to_index(text, self.word2idx)\n",
    "        ### END SOLUTION\n",
    "        # return text as long tensor, labels as float tensor;\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936af50a",
   "metadata": {},
   "source": [
    "### 1.3 Collate Function [10 points]\n",
    "\n",
    "The collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
    "\n",
    "For example, assume the `DataLoader` gets a list of two samples.\n",
    "\n",
    "```\n",
    "[ [3,  1,  2, 8, 5], \n",
    "  [12, 13, 6, 7, 12, 23, 11] ]\n",
    "```\n",
    "\n",
    "where the first sample has text `[3, 1, 2, 8, 5]` the second sample has text `[12, 13, 6, 7, 12, 23, 11]`.\n",
    "\n",
    "The collate function `collate_fn()` is supposed to pad them into the same shape (7), where 7 is the maximum number of tokens.\n",
    "\n",
    "``` \n",
    "[ [3,  1,  2, 8, 5, *0*, *0*], \n",
    "  [12, 13, 6, 7, 12, 23,  11 ]\n",
    "```\n",
    "\n",
    "where `*0*` indicates the padding token.\n",
    "\n",
    "We need to pad the sequences into the same length so that we can do batch training on GPU. And we also need this mask so that when training, we can ignored the padded value as they actually do not contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ea9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: implement the collate function.\n",
    "    \n",
    "    STEP: 1. pad the text using pad_sequence(). Set `batch_first=True`.\n",
    "          2. stack the labels using torch.stack().\n",
    "          \n",
    "    OUTPUT:\n",
    "        text: the padded text, shape: (batch size, max length)\n",
    "        labels: the stacked labels, shape: (batch size, num classes)\n",
    "    \"\"\"\n",
    "    text, labels = zip(*data)\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    text = pad_sequence(text, batch_first=True)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367efff",
   "metadata": {},
   "source": [
    "All done, now let us load the dataset and data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526dce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(f'{DATA_PATH}/train_df.csv')\n",
    "test_set = CustomDataset(f'{DATA_PATH}/test_df.csv')\n",
    "train_loader = DataLoader(train_set, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16369b",
   "metadata": {},
   "source": [
    "## 2 Model\n",
    "\n",
    "Next, we will implement the CAML model.\n",
    "\n",
    "<img src='img/caml.png'>\n",
    "\n",
    "CAML is a convolutional neural network (CNN)-based model. It employs a per-label attention mechanism, which allows the model to learn distinct document representations for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15abca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "\n",
    "class CAML(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size=10, num_filter_maps=16, embed_size=100, dropout=0.5):\n",
    "        super(CAML, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(NUM_WORDS, embed_size, padding_idx=0)\n",
    "        self.embed_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        # initialize conv layer as in section 2.1\n",
    "        self.conv = nn.Conv1d(embed_size, num_filter_maps, kernel_size=kernel_size, padding=int(floor(kernel_size/2)))\n",
    "        xavier_uniform_(self.conv.weight)\n",
    "\n",
    "        # context vectors for computing attention as in section 2.2\n",
    "        self.U = nn.Linear(num_filter_maps, 20)\n",
    "        xavier_uniform_(self.U.weight)\n",
    "\n",
    "        # final layer: create a matrix to use for the NUM_CLASSES binary classifiers as in section 2.3\n",
    "        self.final = nn.Linear(num_filter_maps, NUM_CLASSES)\n",
    "        xavier_uniform_(self.final.weight)\n",
    "        \n",
    "    def forward_embed(self, text):\n",
    "        \"\"\"\n",
    "        TODO: Feed text through the embedding (self.embed) and dropout layer (self.embed_drop).\n",
    "        \n",
    "        INPUT: \n",
    "            text: (batch size, seq_len)\n",
    "            \n",
    "        OURPUT:\n",
    "            text: (batch size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        text = self.embed(text)\n",
    "        text = self.embed_drop(text)\n",
    "        return text\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward_conv(self, text):\n",
    "        \"\"\"\n",
    "        TODO: Feed text through the convolution layer (self.conv) and tanh activation function (torch.tanh) \n",
    "        in eq (1) in the paper.\n",
    "        \n",
    "        INTPUT:\n",
    "            text: (batch size, embed_size, seq_len)\n",
    "            \n",
    "        OUTPUT:\n",
    "            text: (batch size, num_filter_maps, seq_len)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return torch.tanh(self.conv(text))\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward_calc_atten(self, text):\n",
    "        \"\"\"\n",
    "        TODO: calculate the attention weights in eq (2) in the paper. Be sure to read the documentation for\n",
    "        F.softmax()\n",
    "        \n",
    "        INPUT:\n",
    "            text: (batch size, seq_len, num_filter_maps)\n",
    "\n",
    "        OUTPUT:\n",
    "            alpha: (batch size, num_class, seq_len), the attention weights\n",
    "            \n",
    "        STEP: 1. multiply `self.U.weight` with `text` using torch.matmul();\n",
    "              2. apply softmax using `F.softmax()`.\n",
    "        \"\"\"\n",
    "        # (batch size, seq_len, num_filter_maps) -> (batch size, num_filter_mapsseq_len)\n",
    "        text = text.transpose(1,2)\n",
    "        ### BEGIN SOLUTION\n",
    "        return F.softmax(self.U.weight.matmul(text), dim=2)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward_aply_atten(self, alpha, text):\n",
    "        \"\"\"\n",
    "        TODO: apply the attention in eq (3) in the paper.\n",
    "\n",
    "        INPUT: \n",
    "            text: (batch size, seq_len, num_filter_maps)\n",
    "            alpha: (batch size, num_class, seq_len), the attention weights\n",
    "            \n",
    "        OUTPUT:\n",
    "            v: (batch size, num_class, num_filter_maps), vector representations for each label\n",
    "            \n",
    "        STEP: multiply `alpha` with `text` using torch.matmul().\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return alpha.matmul(text)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def forward_linear(self, v):\n",
    "        \"\"\"\n",
    "        TODO: apply the final linear classification in eq (5) in the paper.\n",
    "        \n",
    "        INPUT: \n",
    "            v: (batch size, num_class, num_filter_maps), vector representations for each label\n",
    "            \n",
    "        OUTPUT:\n",
    "            y_hat: (batch size, num_class), label probability\n",
    "            \n",
    "        STEP: 1. multiply `self.final.weight` v `text` element-wise using torch.mul();\n",
    "              2. sum the result over dim 2 (i.e. num_filter_maps);\n",
    "              3. add the result with `self.final.bias`;\n",
    "              4. apply sigmoid with torch.sigmoid().\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        y = self.final.weight.mul(v).sum(dim=2).add(self.final.bias)\n",
    "        y_hat = torch.sigmoid(y)\n",
    "        return y_hat\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \"\"\" 1. get embeddings and apply dropout \"\"\"\n",
    "        text = self.forward_embed(text)\n",
    "        # (batch size, seq_len, embed_size) -> (batch size, embed_size, seq_len);\n",
    "        text = text.transpose(1, 2)\n",
    "\n",
    "        \"\"\" 2. apply convolution and nonlinearity (tanh) \"\"\"\n",
    "        text = self.forward_conv(text)\n",
    "        # (batch size, num_filter_maps, seq_len) -> (batch size, seq_len, num_filter_maps);\n",
    "        text = text.transpose(1,2)\n",
    "        \n",
    "        \"\"\" 3. calculate attention \"\"\"\n",
    "        alpha = self.forward_calc_atten(text)\n",
    "        \n",
    "        \"\"\" 3. apply attention \"\"\"\n",
    "        v = self.forward_aply_atten(alpha, text)\n",
    "           \n",
    "        \"\"\" 4. final layer classification \"\"\"\n",
    "        y_hat = self.forward_linear(v)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "model = CAML()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688b0f17",
   "metadata": {},
   "source": [
    "## 3 Training and Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de1cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CAML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7124294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483aa0e",
   "metadata": {},
   "source": [
    "Now let us implement the `eval()` and `train()` function. Note that `train()` should call `eval()` at the end of each training epoch to see the results on the validaion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f558128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def eval(model, test_loader):\n",
    "    \n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: the CAML model\n",
    "        test_loader: dataloader\n",
    "        \n",
    "    OUTPUT:\n",
    "        precision: overall micro precision score\n",
    "        recall: overall micro recall score\n",
    "        f1: overall micro f1 score\n",
    "        \n",
    "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for sequences, labels in test_loader:\n",
    "        \"\"\"\n",
    "        TODO: 1. preform forward pass\n",
    "              2. obtain the predicted class (0, 1) by comparing forward pass output against 0.5, \n",
    "                 assign the predicted class to y_hat.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        y_hat = model(sequences)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        ### END SOLUTION\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, labels.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afed89ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.457061\n",
      "Epoch: 1 \t Validation p: 0.00, r:0.00, f: 0.00\n",
      "Epoch: 2 \t Training Loss: 0.262114\n",
      "Epoch: 2 \t Validation p: 0.00, r:0.00, f: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaoqiy2/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \t Training Loss: 0.232308\n",
      "Epoch: 3 \t Validation p: 0.00, r:0.00, f: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaoqiy2/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Training Loss: 0.217560\n",
      "Epoch: 4 \t Validation p: 0.83, r:0.15, f: 0.26\n",
      "Epoch: 5 \t Training Loss: 0.203588\n",
      "Epoch: 5 \t Validation p: 0.81, r:0.22, f: 0.35\n",
      "Epoch: 6 \t Training Loss: 0.193507\n",
      "Epoch: 6 \t Validation p: 0.80, r:0.24, f: 0.37\n",
      "Epoch: 7 \t Training Loss: 0.185501\n",
      "Epoch: 7 \t Validation p: 0.83, r:0.25, f: 0.39\n",
      "Epoch: 8 \t Training Loss: 0.180506\n",
      "Epoch: 8 \t Validation p: 0.82, r:0.28, f: 0.42\n",
      "Epoch: 9 \t Training Loss: 0.174325\n",
      "Epoch: 9 \t Validation p: 0.83, r:0.28, f: 0.42\n",
      "Epoch: 10 \t Training Loss: 0.166225\n",
      "Epoch: 10 \t Validation p: 0.86, r:0.32, f: 0.47\n",
      "Epoch: 11 \t Training Loss: 0.157288\n",
      "Epoch: 11 \t Validation p: 0.89, r:0.40, f: 0.55\n",
      "Epoch: 12 \t Training Loss: 0.148467\n",
      "Epoch: 12 \t Validation p: 0.90, r:0.43, f: 0.58\n",
      "Epoch: 13 \t Training Loss: 0.144797\n",
      "Epoch: 13 \t Validation p: 0.89, r:0.47, f: 0.61\n",
      "Epoch: 14 \t Training Loss: 0.136517\n",
      "Epoch: 14 \t Validation p: 0.89, r:0.49, f: 0.64\n",
      "Epoch: 15 \t Training Loss: 0.131915\n",
      "Epoch: 15 \t Validation p: 0.90, r:0.54, f: 0.68\n",
      "Epoch: 16 \t Training Loss: 0.125044\n",
      "Epoch: 16 \t Validation p: 0.90, r:0.59, f: 0.71\n",
      "Epoch: 17 \t Training Loss: 0.120667\n",
      "Epoch: 17 \t Validation p: 0.91, r:0.60, f: 0.72\n",
      "Epoch: 18 \t Training Loss: 0.116765\n",
      "Epoch: 18 \t Validation p: 0.92, r:0.61, f: 0.74\n",
      "Epoch: 19 \t Training Loss: 0.111331\n",
      "Epoch: 19 \t Validation p: 0.92, r:0.64, f: 0.75\n",
      "Epoch: 20 \t Training Loss: 0.108557\n",
      "Epoch: 20 \t Validation p: 0.91, r:0.64, f: 0.75\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"    \n",
    "    INPUT:\n",
    "        model: the CAML model\n",
    "        train_loader: dataloder\n",
    "        val_loader: dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" \n",
    "            TODO: 1. perform forward pass using `model`, save the output to y_hat;\n",
    "                  2. calculate the loss using `criterion`, save the output to loss.\n",
    "            \"\"\"\n",
    "            y_hat, loss = None, None\n",
    "            ### BEGIN SOLUTION\n",
    "            y_hat = model(sequences)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            ### END SOLUTION\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f = eval(model, test_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}'.format(epoch+1, p, r, f))\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "train(model, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef535f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95068d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
