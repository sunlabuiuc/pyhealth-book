{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df7d00c-76f1-448a-8334-f699bf85bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from DeepPurpose.dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff29e28-816b-455f-acb5-df0fdf556575",
   "metadata": {},
   "source": [
    "## 1. load data by deeppurpose\n",
    "- we load the \"AID1706_SARS_CoV_3CL\" dataset by the DeepPurpose package\n",
    "  - The dataset could be found at https://pubchem.ncbi.nlm.nih.gov/bioassay/1706\n",
    "- and select the first 1000 drugs as the toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccfa6a09-6466-4d85-a165-07d69f4ba62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "100% [...............]Default binary threshold for the binding affinity scores is 15, recommended by the investigator\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load AID1706 Assay Data\n",
    "X_drugs, _, y = load_AID1706_SARS_CoV_3CL()\n",
    "\n",
    "# we use the first 1000 molecules for this notebook\n",
    "X_drugs = X_drugs[:1000]\n",
    "y = y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ece9906-b591-4941-972f-8e6a4bde8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC1=C(SC(=N1)NC(=O)COC2=CC=CC=C2OC)C' 'CC1=CC=C(C=C1)C(=O)NCCCN2CCOCC2'\n",
      " 'CSC1=CC=C(C=C1)C(=O)NC2CCSC3=CC=CC=C23'\n",
      " 'CCOC(=O)N1CCC(CC1)N2CC34C=CC(O3)C(C4C2=O)C(=O)NC5=CC=C(C=C5)C'\n",
      " 'CC1=CC(=NN1C(=O)C2=CC(=CC(=C2)[N+](=O)[O-])[N+](=O)[O-])C'\n",
      " 'CC1=CC=C(C=C1)C(=O)CSC2=NN=C(N2CC3=CC=CO3)CNC4=C(C=C(C=C4)C)C'\n",
      " 'CC(C1=CC(=C(C=C1)Cl)Cl)NC(=O)CCl'\n",
      " 'CCOC(=O)CN1CC23C=CC(O2)C(C3C1=O)C(=O)NC4=CC5=C(C=C4)OCO5'\n",
      " 'COC(=O)C1=CC=C(C=C1)COC(=O)C2=CC(=C(N=C2)Cl)Cl'\n",
      " 'C1=CC=C2C(=C1)C=C(C(=O)O2)C3=C(C=C(C=C3)NC(=O)CC4=CC=C(C=C4)Cl)Cl']\n",
      "[0 0 0 0 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# We look at the first 10 SMILES strings\n",
    "print (X_drugs[:10])\n",
    "\n",
    "# We look at the first 10 labels\n",
    "print (y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b192fcb-cc6f-4940-9081-cff7529f1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into 80%: 20%\n",
    "\n",
    "data = pd.DataFrame(np.stack([X_drugs, y]).T, columns = [\"SMILES\", \"Label\"])\n",
    "train, test = data.iloc[:int(len(data)*0.8)], data.iloc[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29150100-2601-42ad-929f-f686923a9414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                SMILES Label\n",
      "0                 CC1=C(SC(=N1)NC(=O)COC2=CC=CC=C2OC)C     0\n",
      "1                      CC1=CC=C(C=C1)C(=O)NCCCN2CCOCC2     0\n",
      "2               CSC1=CC=C(C=C1)C(=O)NC2CCSC3=CC=CC=C23     0\n",
      "3    CCOC(=O)N1CCC(CC1)N2CC34C=CC(O3)C(C4C2=O)C(=O)...     0\n",
      "4    CC1=CC(=NN1C(=O)C2=CC(=CC(=C2)[N+](=O)[O-])[N+...     0\n",
      "..                                                 ...   ...\n",
      "795  COC1=CC=CC=C1N(CC(=O)NC2=CC(=C(C=C2)Cl)C(=O)OC...     1\n",
      "796                       COC1=C(C=C(C=C1Cl)C(=O)NN)Cl     1\n",
      "797  C1CC(C2=CC=CC=C2C1)NC(=O)CCC(=O)N3CCN(CC3)S(=O...     0\n",
      "798  CC(=O)NC1=CC=C(C=C1)N(C(C2=CC=C(C=C2)OC)C(=O)N...     1\n",
      "799  CC(C)N(CC1=CC=CC=C1)CC(COC2=CC=CC3=C2C(=CN3)CC...     0\n",
      "\n",
      "[800 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76007dd8-4577-41ff-a81c-0f9d8e11a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                SMILES Label\n",
      "800             CC1=C(SC(=N1)N)C2=CSC(=N2)NC3=NC=CC=N3     0\n",
      "801   C1=CC(=C(C=C1NC(=O)CN2C=C(N=C2)[N+](=O)[O-])Cl)F     0\n",
      "802      C1C2CC3CC1CC(C2)(C3)C4=CC=C(C=C4)/C=N/NC(=S)N     1\n",
      "803  COC1=CC=C(C=C1)NC(=S)N=NC2=C(N(C3=CC=CC=C32)CC...     0\n",
      "804                   CC(C1=CC(=C(C=C1)Cl)Cl)NC(=O)CCl     1\n",
      "..                                                 ...   ...\n",
      "995  CC1=NC2=C(C(=NN2C(=C1)N3CCN(CC3)CC4=CC=CC=C4)C...     0\n",
      "996  COC1=CC=CC(=C1)C(C(=O)NCC2=CC=CO2)N(CC3=CC=CO3...     1\n",
      "997  COC1=CC(=CC(=C1OC)OC)/C=N\\NC(=O)CN2C3=CC=CC=C3...     1\n",
      "998     CN1C(=CC(=O)N(C1=O)C)NC(=O)C2=C(C=C(C=C2)Cl)Cl     0\n",
      "999  CC(C(=O)NC(=O)NC)OC(=O)C1=C2CC/C(=C\\C3=CC=CS3)...     0\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a776c-9286-4888-8eac-44b2869f49a5",
   "metadata": {},
   "source": [
    "## 2. process the molecule SMILES strings into a graph structure (adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fc404c-8087-4a3b-a132-c4edb3973ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_atoms(mol, atom_dict):\n",
    "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
    "    into the indices (e.g., H=0, C=1, and O=2).\n",
    "    Note that each atom index considers the aromaticity.\n",
    "    \"\"\"\n",
    "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    for a in mol.GetAromaticAtoms():\n",
    "        i = a.GetIdx()\n",
    "        atoms[i] = (atoms[i], 'aromatic')\n",
    "    atoms = [atom_dict[a] for a in atoms]\n",
    "    return np.array(atoms)\n",
    "\n",
    "\n",
    "def create_ijbonddict(mol, bond_dict):\n",
    "    \"\"\"Create a dictionary, in which each key is a node ID\n",
    "    and each value is the tuples of its neighboring node\n",
    "    and chemical bond (e.g., single and double) IDs.\n",
    "    \"\"\"\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "    return i_jbond_dict\n",
    "\n",
    "\n",
    "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                         fingerprint_dict, edge_dict):\n",
    "    \"\"\"Extract the fingerprints from a molecular graph\n",
    "    based on Weisfeiler-Lehman algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        nodes = [fingerprint_dict[a] for a in atoms]\n",
    "\n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "\n",
    "        for _ in range(radius):\n",
    "\n",
    "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
    "            The updated node IDs are the fingerprint IDs.\n",
    "            \"\"\"\n",
    "            nodes_ = []\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
    "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
    "                nodes_.append(fingerprint_dict[fingerprint])\n",
    "\n",
    "            \"\"\"Also update each edge ID considering\n",
    "            its two nodes on both sides.\n",
    "            \"\"\"\n",
    "            i_jedge_dict_ = defaultdict(lambda: [])\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    i_jedge_dict_[i].append((j, edge))\n",
    "\n",
    "            nodes = nodes_\n",
    "            i_jedge_dict = i_jedge_dict_\n",
    "\n",
    "    return np.array(nodes)\n",
    "\n",
    "def create_dataset(data_in, radius=2):\n",
    "    dataset = []\n",
    "\n",
    "    for smiles, property in data_in.values:\n",
    "        try:\n",
    "            \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "            mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "            atoms = create_atoms(mol, atom_dict)\n",
    "            molecular_size = len(atoms)\n",
    "            i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "            fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "            adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "    \n",
    "            \"\"\"Transform the above each data of numpy\n",
    "            to pytorch tensor on a device (i.e., CPU).\n",
    "            \"\"\"\n",
    "            fingerprints = torch.LongTensor(fingerprints)\n",
    "            adjacency = torch.FloatTensor(adjacency)\n",
    "    \n",
    "            dataset.append((fingerprints, adjacency, molecular_size, int(property)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef1fa14-b666-4e24-836a-9e06f5d6c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialize x_dict, in which each key is a symbol type\n",
    "(e.g., atom and chemical bond) and each value is its index.\n",
    "\"\"\"\n",
    "atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "\n",
    "dataset_train = create_dataset(train[[\"SMILES\", \"Label\"]])\n",
    "dataset_test = create_dataset(test[[\"SMILES\", \"Label\"]])\n",
    "\n",
    "N_fingerprints = len(fingerprint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f644e73f-3f2f-49c1-bc0a-4949562c265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 29, 28, 31,\n",
      "        32, 33, 34, 34, 34, 35, 36, 36, 37, 37, 37, 37, 38, 38, 38, 34, 34, 34]), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 36, 0), (tensor([46, 47, 48, 48, 49, 48, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 58,\n",
      "        57, 34, 34, 34, 37, 37, 37, 37, 60, 61, 61, 62, 62, 61, 61, 61, 61, 36,\n",
      "        36, 36, 36, 61, 61]), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 41, 0), (tensor([68, 69, 70, 71, 48, 49, 48, 71, 50, 51, 72, 73, 74, 75, 76, 77, 71, 30,\n",
      "        30, 48, 78, 79, 79, 79, 37, 37, 37, 37, 60, 80, 62, 62, 81, 81, 37, 37,\n",
      "        37, 37]), tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 38, 0)]\n"
     ]
    }
   ],
   "source": [
    "# look at the first 3 data points in training\n",
    "# they are at (fingerprints, adjacency, molecular_size, property) structure\n",
    "print (dataset_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969893-ca5c-4121-a6be-f83f7067bb0d",
   "metadata": {},
   "source": [
    "## 3. Define the Molecule GNN model for molecule structure learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfb4cb6d-3e11-4d22-a327-f8c67c0303d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    borrowed largely from this repo https://github.com/masashitsubaki/molecularGNN_smiles\n",
    "    \"\"\"\n",
    "    def __init__(self, N_fingerprints, dim, layer_gnn_hidden):\n",
    "        super(MolecularGNN, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(N_fingerprints, dim)\n",
    "        self.W_fingerprint = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                            for _ in range(layer_gnn_hidden)])\n",
    "        self.W_property = nn.Linear(dim, 1)\n",
    "\n",
    "    def pad(self, matrices, pad_value):\n",
    "        \"\"\"Pad the list of matrices\n",
    "        with a pad_value (e.g., 0) for batch processing.\n",
    "        For example, given a list of matrices [A, B, C],\n",
    "        we obtain a new matrix [A00, 0B0, 00C],\n",
    "        where 0 is the zero (i.e., pad value) matrix.\n",
    "        \"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        zeros = torch.FloatTensor(np.zeros((M, N)))\n",
    "        pad_matrices = pad_value + zeros\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "\n",
    "    def update(self, matrix, vectors, layer):\n",
    "        hidden_vectors = torch.relu(self.W_fingerprint[layer](vectors))\n",
    "        return hidden_vectors + torch.matmul(matrix, hidden_vectors)\n",
    "\n",
    "    def sum(self, vectors, axis):\n",
    "        sum_vectors = [torch.sum(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(sum_vectors)\n",
    "\n",
    "    def mean(self, vectors, axis):\n",
    "        mean_vectors = [torch.mean(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(mean_vectors)\n",
    "\n",
    "    def gnn(self, inputs):\n",
    "        \"\"\"Cat or pad each input data for batch processing.\"\"\"\n",
    "        fingerprints, adjacencies, molecular_sizes = inputs\n",
    "        fingerprints = torch.cat(fingerprints)\n",
    "        adjacencies = self.pad(adjacencies, 0)\n",
    "\n",
    "        \"\"\"GNN layer (update the fingerprint vectors).\"\"\"\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "        for l in range(len(self.W_fingerprint)):\n",
    "            hs = self.update(adjacencies, fingerprint_vectors, l)\n",
    "            # fingerprint_vectors = F.normalize(hs, 2, 1)  # normalize.\n",
    "\n",
    "        \"\"\"Molecular vector by sum or mean of the fingerprint vectors.\"\"\"\n",
    "        molecular_vectors = self.sum(fingerprint_vectors, molecular_sizes)\n",
    "        # molecular_vectors = self.mean(fingerprint_vectors, molecular_sizes)\n",
    "        return molecular_vectors\n",
    "\n",
    "    def forward(self, data_batch):\n",
    "        molecular_vectors = self.gnn(data_batch)\n",
    "        predicted_scores = self.W_property(molecular_vectors)\n",
    "        return predicted_scores\n",
    "\n",
    "model = MolecularGNN(N_fingerprints, 128, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc746be-5cf3-495c-9017-a855ae61e690",
   "metadata": {},
   "source": [
    "## 4. training the model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e7b366-0f39-42da-b9d2-80d2b1e18449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch: 0 ---, train loss: 39.32982802391052, test AUROC: 0.5515151515151515\n",
      "--- epoch: 1 ---, train loss: 24.156920433044434, test AUROC: 0.6504513140027158\n",
      "--- epoch: 2 ---, train loss: 18.350540339946747, test AUROC: 0.7955907021327583\n",
      "--- epoch: 3 ---, train loss: 15.509063512086868, test AUROC: 0.8324640937174036\n",
      "--- epoch: 4 ---, train loss: 12.810607969760895, test AUROC: 0.8664265706282513\n",
      "--- epoch: 5 ---, train loss: 11.102667361497879, test AUROC: 0.9002656363197294\n",
      "--- epoch: 6 ---, train loss: 9.69138415157795, test AUROC: 0.9347860791826309\n",
      "--- epoch: 7 ---, train loss: 8.683858707547188, test AUROC: 0.9528820856254485\n",
      "--- epoch: 8 ---, train loss: 7.6352120488882065, test AUROC: 0.9483418367346939\n",
      "--- epoch: 9 ---, train loss: 6.850203737616539, test AUROC: 0.969544766004943\n",
      "--- epoch: 10 ---, train loss: 6.049233138561249, test AUROC: 0.9728867623604466\n",
      "--- epoch: 11 ---, train loss: 5.406656213104725, test AUROC: 0.989516129032258\n",
      "--- epoch: 12 ---, train loss: 4.949194274842739, test AUROC: 0.9864766964501328\n",
      "--- epoch: 13 ---, train loss: 4.4005933329463005, test AUROC: 0.9948178266762338\n",
      "--- epoch: 14 ---, train loss: 3.937614791095257, test AUROC: 0.9946519795657727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "batch_size = 32\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(15):\n",
    "    \"\"\" model training \"\"\"\n",
    "    np.random.shuffle(dataset_train)\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for i in range(0, len(dataset_train), batch_size):\n",
    "        data_batch = list(zip(*dataset_train[i: i+batch_size]))\n",
    "        \n",
    "        # feed features into the model\n",
    "        pred = model.forward(data_batch[:3]).squeeze(-1)\n",
    "        \n",
    "        # use gt property as labels\n",
    "        label = torch.FloatTensor(data_batch[-1])\n",
    "        loss = criterion(pred, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    \"\"\" model evaluation \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predicted, groundtruth = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset_test), batch_size):\n",
    "            data_batch = list(zip(*dataset_train[i: i+batch_size]))\n",
    "            # feed features into the model\n",
    "            pred = model.forward(data_batch[:3]).squeeze(-1).numpy().tolist()\n",
    "            label = data_batch[-1]\n",
    "\n",
    "            predicted += pred\n",
    "            groundtruth += label\n",
    "        \n",
    "    print (f\"--- epoch: {epoch} ---, train loss: {train_loss}, test AUROC: {roc_auc_score(groundtruth, predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7379ce-637e-4f39-b51a-f5f1d16f4a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
