2023-12-20 21:59:56 VAE(
  (encoder1): Linear(in_features=1024, out_features=64, bias=True)
  (mu): Linear(in_features=64, out_features=64, bias=True)
  (log_std2): Linear(in_features=64, out_features=64, bias=True)
  (decoder1): Linear(in_features=64, out_features=64, bias=True)
  (decoder2): Linear(in_features=64, out_features=1024, bias=True)
)
2023-12-20 21:59:56 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-20 21:59:56 Device: cuda:4
2023-12-20 21:59:56 
2023-12-20 21:59:56 Training:
2023-12-20 21:59:56 Batch size: 32
2023-12-20 21:59:56 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-20 21:59:56 Optimizer params: {'lr': 0.001}
2023-12-20 21:59:56 Weight decay: 0.0
2023-12-20 21:59:56 Max grad norm: None
2023-12-20 21:59:56 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9c748e3cd0>
2023-12-20 21:59:56 Monitor: None
2023-12-20 21:59:56 Monitor criterion: max
2023-12-20 21:59:56 Epochs: 20
2023-12-20 21:59:56 
2023-12-20 22:00:17 --- Train epoch-0, step-397 ---
2023-12-20 22:00:17 loss: 20612.2243
2023-12-20 22:00:23 --- Eval epoch-0, step-397 ---
2023-12-20 22:00:23 kl_divergence: 0.2122
2023-12-20 22:00:23 mse: 0.0000
2023-12-20 22:00:23 mae: 0.0000
2023-12-20 22:00:23 loss: 20030.6153
2023-12-20 22:00:23 
2023-12-20 22:00:42 --- Train epoch-1, step-794 ---
2023-12-20 22:00:42 loss: 19935.1167
2023-12-20 22:00:48 --- Eval epoch-1, step-794 ---
2023-12-20 22:00:48 kl_divergence: 0.1778
2023-12-20 22:00:48 mse: 0.0000
2023-12-20 22:00:48 mae: 0.0000
2023-12-20 22:00:48 loss: 19738.4888
2023-12-20 22:00:48 
2023-12-20 22:01:08 --- Train epoch-2, step-1191 ---
2023-12-20 22:01:08 loss: 19679.2131
2023-12-20 22:01:14 --- Eval epoch-2, step-1191 ---
2023-12-20 22:01:14 kl_divergence: 0.1541
2023-12-20 22:01:14 mse: 0.0000
2023-12-20 22:01:14 mae: 0.0000
2023-12-20 22:01:14 loss: 19548.9558
2023-12-20 22:01:14 
2023-12-20 22:01:33 --- Train epoch-3, step-1588 ---
2023-12-20 22:01:33 loss: 19544.7695
2023-12-20 22:01:39 --- Eval epoch-3, step-1588 ---
2023-12-20 22:01:39 kl_divergence: 0.1318
2023-12-20 22:01:39 mse: 0.0000
2023-12-20 22:01:39 mae: 0.0000
2023-12-20 22:01:39 loss: 19378.9099
2023-12-20 22:01:39 
2023-12-20 22:01:58 --- Train epoch-4, step-1985 ---
2023-12-20 22:01:58 loss: 19443.5065
2023-12-20 22:02:04 --- Eval epoch-4, step-1985 ---
2023-12-20 22:02:04 kl_divergence: 0.1255
2023-12-20 22:02:04 mse: 0.0000
2023-12-20 22:02:04 mae: 0.0000
2023-12-20 22:02:04 loss: 19308.7676
2023-12-20 22:02:04 
2023-12-20 22:02:22 --- Train epoch-5, step-2382 ---
2023-12-20 22:02:22 loss: 19393.5087
2023-12-20 22:02:28 --- Eval epoch-5, step-2382 ---
2023-12-20 22:02:28 kl_divergence: 0.1242
2023-12-20 22:02:28 mse: 0.0000
2023-12-20 22:02:28 mae: 0.0000
2023-12-20 22:02:28 loss: 19279.4748
2023-12-20 22:02:28 
2023-12-20 22:02:47 --- Train epoch-6, step-2779 ---
2023-12-20 22:02:47 loss: 19371.4925
2023-12-20 22:02:54 --- Eval epoch-6, step-2779 ---
2023-12-20 22:02:54 kl_divergence: 0.1160
2023-12-20 22:02:54 mse: 0.0000
2023-12-20 22:02:54 mae: 0.0000
2023-12-20 22:02:54 loss: 19240.6900
2023-12-20 22:02:54 
2023-12-20 22:03:13 --- Train epoch-7, step-3176 ---
2023-12-20 22:03:13 loss: 19344.3754
2023-12-20 22:03:20 --- Eval epoch-7, step-3176 ---
2023-12-20 22:03:20 kl_divergence: 0.1221
2023-12-20 22:03:20 mse: 0.0000
2023-12-20 22:03:20 mae: 0.0000
2023-12-20 22:03:20 loss: 19226.0985
2023-12-20 22:03:20 
2023-12-20 22:03:39 --- Train epoch-8, step-3573 ---
2023-12-20 22:03:39 loss: 19331.6667
2023-12-20 22:03:46 --- Eval epoch-8, step-3573 ---
2023-12-20 22:03:46 kl_divergence: 0.1149
2023-12-20 22:03:46 mse: 0.0000
2023-12-20 22:03:46 mae: 0.0000
2023-12-20 22:03:46 loss: 19209.6395
2023-12-20 22:03:46 
2023-12-20 22:04:06 --- Train epoch-9, step-3970 ---
2023-12-20 22:04:06 loss: 19308.5464
2023-12-20 22:04:13 --- Eval epoch-9, step-3970 ---
2023-12-20 22:04:13 kl_divergence: 0.1099
2023-12-20 22:04:13 mse: 0.0000
2023-12-20 22:04:13 mae: 0.0000
2023-12-20 22:04:13 loss: 19194.9661
2023-12-20 22:04:13 
2023-12-20 22:04:33 --- Train epoch-10, step-4367 ---
2023-12-20 22:04:33 loss: 19298.9741
2023-12-20 22:04:39 --- Eval epoch-10, step-4367 ---
2023-12-20 22:04:39 kl_divergence: 0.1137
2023-12-20 22:04:39 mse: 0.0000
2023-12-20 22:04:39 mae: 0.0000
2023-12-20 22:04:39 loss: 19183.3762
2023-12-20 22:04:39 
2023-12-20 22:04:59 --- Train epoch-11, step-4764 ---
2023-12-20 22:04:59 loss: 19285.9320
2023-12-20 22:05:05 --- Eval epoch-11, step-4764 ---
2023-12-20 22:05:05 kl_divergence: 0.1077
2023-12-20 22:05:05 mse: 0.0000
2023-12-20 22:05:05 mae: 0.0000
2023-12-20 22:05:05 loss: 19182.8189
2023-12-20 22:05:05 
2023-12-20 22:05:25 --- Train epoch-12, step-5161 ---
2023-12-20 22:05:25 loss: 19276.4275
2023-12-20 22:05:31 --- Eval epoch-12, step-5161 ---
2023-12-20 22:05:31 kl_divergence: 0.1053
2023-12-20 22:05:31 mse: 0.0000
2023-12-20 22:05:31 mae: 0.0000
2023-12-20 22:05:31 loss: 19168.4431
2023-12-20 22:05:31 
2023-12-20 22:05:52 --- Train epoch-13, step-5558 ---
2023-12-20 22:05:52 loss: 19271.8524
2023-12-20 22:05:59 --- Eval epoch-13, step-5558 ---
2023-12-20 22:05:59 kl_divergence: 0.1050
2023-12-20 22:05:59 mse: 0.0000
2023-12-20 22:05:59 mae: 0.0000
2023-12-20 22:05:59 loss: 19161.5382
2023-12-20 22:05:59 
2023-12-20 22:06:19 --- Train epoch-14, step-5955 ---
2023-12-20 22:06:19 loss: 19263.5545
2023-12-20 22:06:26 --- Eval epoch-14, step-5955 ---
2023-12-20 22:06:26 kl_divergence: 0.1052
2023-12-20 22:06:26 mse: 0.0000
2023-12-20 22:06:26 mae: 0.0000
2023-12-20 22:06:26 loss: 19157.3488
2023-12-20 22:06:26 
2023-12-20 22:06:47 --- Train epoch-15, step-6352 ---
2023-12-20 22:06:47 loss: 19251.7475
2023-12-20 22:06:54 --- Eval epoch-15, step-6352 ---
2023-12-20 22:06:54 kl_divergence: 0.0985
2023-12-20 22:06:54 mse: 0.0000
2023-12-20 22:06:54 mae: 0.0000
2023-12-20 22:06:54 loss: 19159.6862
2023-12-20 22:06:54 
2023-12-20 22:07:15 --- Train epoch-16, step-6749 ---
2023-12-20 22:07:15 loss: 19242.6107
2023-12-20 22:07:22 --- Eval epoch-16, step-6749 ---
2023-12-20 22:07:22 kl_divergence: 0.0964
2023-12-20 22:07:22 mse: 0.0000
2023-12-20 22:07:22 mae: 0.0000
2023-12-20 22:07:22 loss: 19133.7823
2023-12-20 22:07:22 
2023-12-20 22:07:43 --- Train epoch-17, step-7146 ---
2023-12-20 22:07:43 loss: 19234.4084
2023-12-20 22:07:50 --- Eval epoch-17, step-7146 ---
2023-12-20 22:07:50 kl_divergence: 0.0989
2023-12-20 22:07:50 mse: 0.0000
2023-12-20 22:07:50 mae: 0.0000
2023-12-20 22:07:50 loss: 19127.3581
2023-12-20 22:07:50 
2023-12-20 22:08:11 --- Train epoch-18, step-7543 ---
2023-12-20 22:08:11 loss: 19229.1087
2023-12-20 22:08:18 --- Eval epoch-18, step-7543 ---
2023-12-20 22:08:18 kl_divergence: 0.1048
2023-12-20 22:08:18 mse: 0.0000
2023-12-20 22:08:18 mae: 0.0000
2023-12-20 22:08:18 loss: 19123.9767
2023-12-20 22:08:18 
2023-12-20 22:08:39 --- Train epoch-19, step-7940 ---
2023-12-20 22:08:39 loss: 19221.1448
2023-12-20 22:08:46 --- Eval epoch-19, step-7940 ---
2023-12-20 22:08:46 kl_divergence: 0.0985
2023-12-20 22:08:46 mse: 0.0000
2023-12-20 22:08:46 mae: 0.0000
2023-12-20 22:08:46 loss: 19112.7360
2023-12-21 14:06:47 VAE(
  (encoder1): Linear(in_features=16384, out_features=256, bias=True)
  (mu): Linear(in_features=256, out_features=256, bias=True)
  (log_std2): Linear(in_features=256, out_features=256, bias=True)
  (decoder1): Linear(in_features=256, out_features=256, bias=True)
  (decoder2): Linear(in_features=256, out_features=16384, bias=True)
)
2023-12-21 14:06:47 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-21 14:06:47 Device: cuda:4
2023-12-21 14:06:47 
2023-12-21 14:06:47 Training:
2023-12-21 14:06:47 Batch size: 32
2023-12-21 14:06:47 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-21 14:06:47 Optimizer params: {'lr': 0.002}
2023-12-21 14:06:47 Weight decay: 0.0
2023-12-21 14:06:47 Max grad norm: None
2023-12-21 14:06:47 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9afa3bf710>
2023-12-21 14:06:47 Monitor: None
2023-12-21 14:06:47 Monitor criterion: max
2023-12-21 14:06:47 Epochs: 20
2023-12-21 14:06:47 
2023-12-21 14:07:14 --- Train epoch-0, step-397 ---
2023-12-21 14:07:14 loss: 336386.2539
2023-12-21 14:07:24 --- Eval epoch-0, step-397 ---
2023-12-21 14:07:24 kl_divergence: 0.3435
2023-12-21 14:07:24 mse: 0.0000
2023-12-21 14:07:24 mae: 0.0000
2023-12-21 14:07:24 loss: 333606.8191
2023-12-21 14:07:24 
2023-12-21 14:07:48 --- Train epoch-1, step-794 ---
2023-12-21 14:07:48 loss: 335969.9654
2023-12-21 14:07:59 --- Eval epoch-1, step-794 ---
2023-12-21 14:07:59 kl_divergence: 0.3436
2023-12-21 14:07:59 mse: 0.0000
2023-12-21 14:07:59 mae: 0.0000
2023-12-21 14:07:59 loss: 333421.0774
2023-12-21 14:07:59 
2023-12-21 14:08:23 --- Train epoch-2, step-1191 ---
2023-12-21 14:08:23 loss: 336024.0534
2023-12-21 14:08:33 --- Eval epoch-2, step-1191 ---
2023-12-21 14:08:33 kl_divergence: 0.3268
2023-12-21 14:08:33 mse: 0.0000
2023-12-21 14:08:33 mae: 0.0000
2023-12-21 14:08:33 loss: 333984.2209
2023-12-21 14:08:33 
2023-12-21 14:08:55 VAE(
  (encoder1): Linear(in_features=16384, out_features=256, bias=True)
  (mu): Linear(in_features=256, out_features=256, bias=True)
  (log_std2): Linear(in_features=256, out_features=256, bias=True)
  (decoder1): Linear(in_features=256, out_features=256, bias=True)
  (decoder2): Linear(in_features=256, out_features=16384, bias=True)
)
2023-12-21 14:08:55 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-21 14:08:55 Device: cuda:4
2023-12-21 14:08:55 
2023-12-21 14:08:55 Training:
2023-12-21 14:08:55 Batch size: 256
2023-12-21 14:08:55 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-21 14:08:55 Optimizer params: {'lr': 0.001}
2023-12-21 14:08:55 Weight decay: 0.0
2023-12-21 14:08:55 Max grad norm: None
2023-12-21 14:08:55 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9afa353cd0>
2023-12-21 14:08:55 Monitor: None
2023-12-21 14:08:55 Monitor criterion: max
2023-12-21 14:08:55 Epochs: 20
2023-12-21 14:08:55 
2023-12-21 14:09:27 --- Train epoch-0, step-67 ---
2023-12-21 14:09:27 loss: 2681509.8148
2023-12-21 14:09:34 --- Eval epoch-0, step-67 ---
2023-12-21 14:09:34 kl_divergence: 0.3379
2023-12-21 14:09:34 mse: 0.0000
2023-12-21 14:09:34 mae: 0.0000
2023-12-21 14:09:34 loss: 2458973.3681
2023-12-21 14:09:34 
2023-12-21 14:10:05 --- Train epoch-1, step-134 ---
2023-12-21 14:10:05 loss: 2627045.2276
2023-12-21 14:10:10 --- Eval epoch-1, step-134 ---
2023-12-21 14:10:10 kl_divergence: 0.3050
2023-12-21 14:10:10 mse: 0.0000
2023-12-21 14:10:10 mae: 0.0000
2023-12-21 14:10:10 loss: 2479773.4653
2023-12-21 14:10:10 
2023-12-21 14:10:40 --- Train epoch-2, step-201 ---
2023-12-21 14:10:40 loss: 2614674.6469
2023-12-21 14:10:45 --- Eval epoch-2, step-201 ---
2023-12-21 14:10:45 kl_divergence: 0.2863
2023-12-21 14:10:45 mse: 0.0000
2023-12-21 14:10:45 mae: 0.0000
2023-12-21 14:10:45 loss: 2424683.5833
2023-12-21 14:10:45 
2023-12-21 14:11:15 --- Train epoch-3, step-268 ---
2023-12-21 14:11:15 loss: 2597313.6432
2023-12-21 14:11:19 --- Eval epoch-3, step-268 ---
2023-12-21 14:11:19 kl_divergence: 0.2757
2023-12-21 14:11:19 mse: 0.0000
2023-12-21 14:11:19 mae: 0.0000
2023-12-21 14:11:19 loss: 2422043.2292
2023-12-21 14:11:19 
2023-12-21 14:11:28 VAE(
  (encoder1): Linear(in_features=16384, out_features=256, bias=True)
  (mu): Linear(in_features=256, out_features=256, bias=True)
  (log_std2): Linear(in_features=256, out_features=256, bias=True)
  (decoder1): Linear(in_features=256, out_features=256, bias=True)
  (decoder2): Linear(in_features=256, out_features=16384, bias=True)
)
2023-12-21 14:11:28 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-21 14:11:28 Device: cuda:4
2023-12-21 14:11:28 
2023-12-21 14:11:28 Training:
2023-12-21 14:11:28 Batch size: 256
2023-12-21 14:11:28 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-21 14:11:28 Optimizer params: {'lr': 0.001}
2023-12-21 14:11:28 Weight decay: 0.0
2023-12-21 14:11:28 Max grad norm: None
2023-12-21 14:11:28 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9afa353cd0>
2023-12-21 14:11:28 Monitor: kl_divergence
2023-12-21 14:11:28 Monitor criterion: min
2023-12-21 14:11:28 Epochs: 20
2023-12-21 14:11:28 
2023-12-21 14:11:59 --- Train epoch-0, step-67 ---
2023-12-21 14:11:59 loss: 2591478.8377
2023-12-21 14:12:03 --- Eval epoch-0, step-67 ---
2023-12-21 14:12:03 kl_divergence: 0.2686
2023-12-21 14:12:03 mse: 0.0000
2023-12-21 14:12:03 mae: 0.0000
2023-12-21 14:12:03 loss: 2399905.6875
2023-12-21 14:12:03 New best kl_divergence score (0.2686) at epoch-0, step-67
2023-12-21 14:12:03 
2023-12-21 14:12:35 --- Train epoch-1, step-134 ---
2023-12-21 14:12:35 loss: 2561201.9114
2023-12-21 14:12:40 --- Eval epoch-1, step-134 ---
2023-12-21 14:12:40 kl_divergence: 0.2479
2023-12-21 14:12:40 mse: 0.0000
2023-12-21 14:12:40 mae: 0.0000
2023-12-21 14:12:40 loss: 2392571.9028
2023-12-21 14:12:40 New best kl_divergence score (0.2479) at epoch-1, step-134
2023-12-21 14:12:40 
2023-12-21 14:13:11 --- Train epoch-2, step-201 ---
2023-12-21 14:13:11 loss: 2551366.5690
2023-12-21 14:13:15 --- Eval epoch-2, step-201 ---
2023-12-21 14:13:15 kl_divergence: 0.2202
2023-12-21 14:13:15 mse: 0.0000
2023-12-21 14:13:15 mae: 0.0000
2023-12-21 14:13:15 loss: 2375926.2986
2023-12-21 14:13:15 New best kl_divergence score (0.2202) at epoch-2, step-201
2023-12-21 14:13:15 
2023-12-21 14:13:46 --- Train epoch-3, step-268 ---
2023-12-21 14:13:46 loss: 2545938.6562
2023-12-21 14:13:50 --- Eval epoch-3, step-268 ---
2023-12-21 14:13:50 kl_divergence: 0.2108
2023-12-21 14:13:50 mse: 0.0000
2023-12-21 14:13:50 mae: 0.0000
2023-12-21 14:13:50 loss: 2372509.5208
2023-12-21 14:13:50 New best kl_divergence score (0.2108) at epoch-3, step-268
2023-12-21 14:13:50 
2023-12-21 14:14:21 --- Train epoch-4, step-335 ---
2023-12-21 14:14:21 loss: 2543004.9650
2023-12-21 14:14:26 --- Eval epoch-4, step-335 ---
2023-12-21 14:14:26 kl_divergence: 0.2100
2023-12-21 14:14:26 mse: 0.0000
2023-12-21 14:14:26 mae: 0.0000
2023-12-21 14:14:26 loss: 2375713.4028
2023-12-21 14:14:26 New best kl_divergence score (0.2100) at epoch-4, step-335
2023-12-21 14:14:26 
2023-12-21 14:14:57 --- Train epoch-5, step-402 ---
2023-12-21 14:14:57 loss: 2543365.7220
2023-12-21 14:15:02 --- Eval epoch-5, step-402 ---
2023-12-21 14:15:02 kl_divergence: 0.2149
2023-12-21 14:15:02 mse: 0.0000
2023-12-21 14:15:02 mae: 0.0000
2023-12-21 14:15:02 loss: 2368120.5278
2023-12-21 14:15:02 
2023-12-21 14:15:33 --- Train epoch-6, step-469 ---
2023-12-21 14:15:33 loss: 2538756.8396
2023-12-21 14:15:37 --- Eval epoch-6, step-469 ---
2023-12-21 14:15:37 kl_divergence: 0.2147
2023-12-21 14:15:37 mse: 0.0000
2023-12-21 14:15:37 mae: 0.0000
2023-12-21 14:15:37 loss: 2365206.3333
2023-12-21 14:15:37 
2023-12-21 14:16:08 --- Train epoch-7, step-536 ---
2023-12-21 14:16:08 loss: 2534320.1446
2023-12-21 14:16:13 --- Eval epoch-7, step-536 ---
2023-12-21 14:16:13 kl_divergence: 0.2055
2023-12-21 14:16:13 mse: 0.0000
2023-12-21 14:16:13 mae: 0.0000
2023-12-21 14:16:13 loss: 2360552.0833
2023-12-21 14:16:13 New best kl_divergence score (0.2055) at epoch-7, step-536
2023-12-21 14:16:13 
2023-12-21 14:16:43 --- Train epoch-8, step-603 ---
2023-12-21 14:16:43 loss: 2533359.4688
2023-12-21 14:16:47 --- Eval epoch-8, step-603 ---
2023-12-21 14:16:47 kl_divergence: 0.2095
2023-12-21 14:16:47 mse: 0.0000
2023-12-21 14:16:47 mae: 0.0000
2023-12-21 14:16:47 loss: 2370273.0278
2023-12-21 14:16:47 
2023-12-21 14:17:17 --- Train epoch-9, step-670 ---
2023-12-21 14:17:17 loss: 2532307.4701
2023-12-21 14:17:21 --- Eval epoch-9, step-670 ---
2023-12-21 14:17:21 kl_divergence: 0.2089
2023-12-21 14:17:21 mse: 0.0000
2023-12-21 14:17:21 mae: 0.0000
2023-12-21 14:17:21 loss: 2361183.9653
2023-12-21 14:17:21 
2023-12-21 14:17:52 --- Train epoch-10, step-737 ---
2023-12-21 14:17:52 loss: 2533572.8083
2023-12-21 14:17:56 --- Eval epoch-10, step-737 ---
2023-12-21 14:17:56 kl_divergence: 0.1970
2023-12-21 14:17:56 mse: 0.0000
2023-12-21 14:17:56 mae: 0.0000
2023-12-21 14:17:56 loss: 2360903.3403
2023-12-21 14:17:56 New best kl_divergence score (0.1970) at epoch-10, step-737
2023-12-21 14:17:56 
2023-12-21 14:18:27 --- Train epoch-11, step-804 ---
2023-12-21 14:18:27 loss: 2530745.7491
2023-12-21 14:18:31 --- Eval epoch-11, step-804 ---
2023-12-21 14:18:31 kl_divergence: 0.2119
2023-12-21 14:18:31 mse: 0.0000
2023-12-21 14:18:31 mae: 0.0000
2023-12-21 14:18:31 loss: 2363600.3750
2023-12-21 14:18:31 
2023-12-21 14:19:03 --- Train epoch-12, step-871 ---
2023-12-21 14:19:03 loss: 2530489.0667
2023-12-21 14:19:08 --- Eval epoch-12, step-871 ---
2023-12-21 14:19:08 kl_divergence: 0.2010
2023-12-21 14:19:08 mse: 0.0000
2023-12-21 14:19:08 mae: 0.0000
2023-12-21 14:19:08 loss: 2365807.5972
2023-12-21 14:19:08 
2023-12-21 14:19:38 --- Train epoch-13, step-938 ---
2023-12-21 14:19:38 loss: 2531198.8797
2023-12-21 14:19:42 --- Eval epoch-13, step-938 ---
2023-12-21 14:19:42 kl_divergence: 0.2035
2023-12-21 14:19:42 mse: 0.0000
2023-12-21 14:19:42 mae: 0.0000
2023-12-21 14:19:42 loss: 2360327.6667
2023-12-21 14:19:42 
2023-12-21 14:20:12 --- Train epoch-14, step-1005 ---
2023-12-21 14:20:12 loss: 2530045.4813
2023-12-21 14:20:17 --- Eval epoch-14, step-1005 ---
2023-12-21 14:20:17 kl_divergence: 0.1983
2023-12-21 14:20:17 mse: 0.0000
2023-12-21 14:20:17 mae: 0.0000
2023-12-21 14:20:17 loss: 2357692.1806
2023-12-21 14:20:17 
2023-12-21 14:20:49 --- Train epoch-15, step-1072 ---
2023-12-21 14:20:49 loss: 2530552.7066
2023-12-21 14:20:54 --- Eval epoch-15, step-1072 ---
2023-12-21 14:20:54 kl_divergence: 0.1977
2023-12-21 14:20:54 mse: 0.0000
2023-12-21 14:20:54 mae: 0.0000
2023-12-21 14:20:54 loss: 2364016.3542
2023-12-21 14:20:54 
2023-12-21 14:21:26 --- Train epoch-16, step-1139 ---
2023-12-21 14:21:26 loss: 2529993.7598
2023-12-21 14:21:30 --- Eval epoch-16, step-1139 ---
2023-12-21 14:21:30 kl_divergence: 0.2001
2023-12-21 14:21:30 mse: 0.0000
2023-12-21 14:21:30 mae: 0.0000
2023-12-21 14:21:30 loss: 2357103.1806
2023-12-21 14:21:30 
2023-12-21 14:22:00 --- Train epoch-17, step-1206 ---
2023-12-21 14:22:00 loss: 2528716.8046
2023-12-21 14:22:05 --- Eval epoch-17, step-1206 ---
2023-12-21 14:22:05 kl_divergence: 0.2148
2023-12-21 14:22:05 mse: 0.0000
2023-12-21 14:22:05 mae: 0.0000
2023-12-21 14:22:05 loss: 2363072.6389
2023-12-21 14:22:05 
2023-12-21 14:22:37 --- Train epoch-18, step-1273 ---
2023-12-21 14:22:37 loss: 2528853.2313
2023-12-21 14:22:41 --- Eval epoch-18, step-1273 ---
2023-12-21 14:22:41 kl_divergence: 0.2090
2023-12-21 14:22:41 mse: 0.0000
2023-12-21 14:22:41 mae: 0.0000
2023-12-21 14:22:41 loss: 2359890.1806
2023-12-21 14:22:41 
2023-12-21 14:23:12 --- Train epoch-19, step-1340 ---
2023-12-21 14:23:12 loss: 2530428.1087
2023-12-21 14:23:16 --- Eval epoch-19, step-1340 ---
2023-12-21 14:23:16 kl_divergence: 0.2061
2023-12-21 14:23:16 mse: 0.0000
2023-12-21 14:23:16 mae: 0.0000
2023-12-21 14:23:16 loss: 2358671.3264
2023-12-21 14:23:16 Loaded best model
