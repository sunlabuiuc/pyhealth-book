2023-12-21 14:08:55 VAE(
  (encoder1): Linear(in_features=16384, out_features=256, bias=True)
  (mu): Linear(in_features=256, out_features=256, bias=True)
  (log_std2): Linear(in_features=256, out_features=256, bias=True)
  (decoder1): Linear(in_features=256, out_features=256, bias=True)
  (decoder2): Linear(in_features=256, out_features=16384, bias=True)
)
2023-12-21 14:08:55 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-21 14:08:55 Device: cuda:4
2023-12-21 14:08:55 
2023-12-21 14:08:55 Training:
2023-12-21 14:08:55 Batch size: 256
2023-12-21 14:08:55 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-21 14:08:55 Optimizer params: {'lr': 0.001}
2023-12-21 14:08:55 Weight decay: 0.0
2023-12-21 14:08:55 Max grad norm: None
2023-12-21 14:08:55 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9afa353cd0>
2023-12-21 14:08:55 Monitor: None
2023-12-21 14:08:55 Monitor criterion: max
2023-12-21 14:08:55 Epochs: 20
2023-12-21 14:08:55 
2023-12-21 14:09:27 --- Train epoch-0, step-67 ---
2023-12-21 14:09:27 loss: 2681509.8148
2023-12-21 14:09:34 --- Eval epoch-0, step-67 ---
2023-12-21 14:09:34 kl_divergence: 0.3379
2023-12-21 14:09:34 mse: 0.0000
2023-12-21 14:09:34 mae: 0.0000
2023-12-21 14:09:34 loss: 2458973.3681
2023-12-21 14:09:34 
2023-12-21 14:10:05 --- Train epoch-1, step-134 ---
2023-12-21 14:10:05 loss: 2627045.2276
2023-12-21 14:10:10 --- Eval epoch-1, step-134 ---
2023-12-21 14:10:10 kl_divergence: 0.3050
2023-12-21 14:10:10 mse: 0.0000
2023-12-21 14:10:10 mae: 0.0000
2023-12-21 14:10:10 loss: 2479773.4653
2023-12-21 14:10:10 
2023-12-21 14:10:40 --- Train epoch-2, step-201 ---
2023-12-21 14:10:40 loss: 2614674.6469
2023-12-21 14:10:45 --- Eval epoch-2, step-201 ---
2023-12-21 14:10:45 kl_divergence: 0.2863
2023-12-21 14:10:45 mse: 0.0000
2023-12-21 14:10:45 mae: 0.0000
2023-12-21 14:10:45 loss: 2424683.5833
2023-12-21 14:10:45 
2023-12-21 14:11:15 --- Train epoch-3, step-268 ---
2023-12-21 14:11:15 loss: 2597313.6432
2023-12-21 14:11:19 --- Eval epoch-3, step-268 ---
2023-12-21 14:11:19 kl_divergence: 0.2757
2023-12-21 14:11:19 mse: 0.0000
2023-12-21 14:11:19 mae: 0.0000
2023-12-21 14:11:19 loss: 2422043.2292
2023-12-21 14:11:19 
2023-12-21 14:11:28 VAE(
  (encoder1): Linear(in_features=16384, out_features=256, bias=True)
  (mu): Linear(in_features=256, out_features=256, bias=True)
  (log_std2): Linear(in_features=256, out_features=256, bias=True)
  (decoder1): Linear(in_features=256, out_features=256, bias=True)
  (decoder2): Linear(in_features=256, out_features=16384, bias=True)
)
2023-12-21 14:11:28 Metrics: ['kl_divergence', 'mse', 'mae']
2023-12-21 14:11:28 Device: cuda:4
2023-12-21 14:11:28 
2023-12-21 14:11:28 Training:
2023-12-21 14:11:28 Batch size: 256
2023-12-21 14:11:28 Optimizer: <class 'torch.optim.adam.Adam'>
2023-12-21 14:11:28 Optimizer params: {'lr': 0.001}
2023-12-21 14:11:28 Weight decay: 0.0
2023-12-21 14:11:28 Max grad norm: None
2023-12-21 14:11:28 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f9afa353cd0>
2023-12-21 14:11:28 Monitor: kl_divergence
2023-12-21 14:11:28 Monitor criterion: min
2023-12-21 14:11:28 Epochs: 20
2023-12-21 14:11:28 
2023-12-21 14:11:59 --- Train epoch-0, step-67 ---
2023-12-21 14:11:59 loss: 2591478.8377
2023-12-21 14:12:03 --- Eval epoch-0, step-67 ---
2023-12-21 14:12:03 kl_divergence: 0.2686
2023-12-21 14:12:03 mse: 0.0000
2023-12-21 14:12:03 mae: 0.0000
2023-12-21 14:12:03 loss: 2399905.6875
2023-12-21 14:12:03 New best kl_divergence score (0.2686) at epoch-0, step-67
2023-12-21 14:12:03 
2023-12-21 14:12:35 --- Train epoch-1, step-134 ---
2023-12-21 14:12:35 loss: 2561201.9114
2023-12-21 14:12:40 --- Eval epoch-1, step-134 ---
2023-12-21 14:12:40 kl_divergence: 0.2479
2023-12-21 14:12:40 mse: 0.0000
2023-12-21 14:12:40 mae: 0.0000
2023-12-21 14:12:40 loss: 2392571.9028
2023-12-21 14:12:40 New best kl_divergence score (0.2479) at epoch-1, step-134
2023-12-21 14:12:40 
2023-12-21 14:13:11 --- Train epoch-2, step-201 ---
2023-12-21 14:13:11 loss: 2551366.5690
2023-12-21 14:13:15 --- Eval epoch-2, step-201 ---
2023-12-21 14:13:15 kl_divergence: 0.2202
2023-12-21 14:13:15 mse: 0.0000
2023-12-21 14:13:15 mae: 0.0000
2023-12-21 14:13:15 loss: 2375926.2986
2023-12-21 14:13:15 New best kl_divergence score (0.2202) at epoch-2, step-201
2023-12-21 14:13:15 
2023-12-21 14:13:46 --- Train epoch-3, step-268 ---
2023-12-21 14:13:46 loss: 2545938.6562
2023-12-21 14:13:50 --- Eval epoch-3, step-268 ---
2023-12-21 14:13:50 kl_divergence: 0.2108
2023-12-21 14:13:50 mse: 0.0000
2023-12-21 14:13:50 mae: 0.0000
2023-12-21 14:13:50 loss: 2372509.5208
2023-12-21 14:13:50 New best kl_divergence score (0.2108) at epoch-3, step-268
2023-12-21 14:13:50 
2023-12-21 14:14:21 --- Train epoch-4, step-335 ---
2023-12-21 14:14:21 loss: 2543004.9650
2023-12-21 14:14:26 --- Eval epoch-4, step-335 ---
2023-12-21 14:14:26 kl_divergence: 0.2100
2023-12-21 14:14:26 mse: 0.0000
2023-12-21 14:14:26 mae: 0.0000
2023-12-21 14:14:26 loss: 2375713.4028
2023-12-21 14:14:26 New best kl_divergence score (0.2100) at epoch-4, step-335
2023-12-21 14:14:26 
2023-12-21 14:14:57 --- Train epoch-5, step-402 ---
2023-12-21 14:14:57 loss: 2543365.7220
2023-12-21 14:15:02 --- Eval epoch-5, step-402 ---
2023-12-21 14:15:02 kl_divergence: 0.2149
2023-12-21 14:15:02 mse: 0.0000
2023-12-21 14:15:02 mae: 0.0000
2023-12-21 14:15:02 loss: 2368120.5278
2023-12-21 14:15:02 
2023-12-21 14:15:33 --- Train epoch-6, step-469 ---
2023-12-21 14:15:33 loss: 2538756.8396
2023-12-21 14:15:37 --- Eval epoch-6, step-469 ---
2023-12-21 14:15:37 kl_divergence: 0.2147
2023-12-21 14:15:37 mse: 0.0000
2023-12-21 14:15:37 mae: 0.0000
2023-12-21 14:15:37 loss: 2365206.3333
2023-12-21 14:15:37 
2023-12-21 14:16:08 --- Train epoch-7, step-536 ---
2023-12-21 14:16:08 loss: 2534320.1446
2023-12-21 14:16:13 --- Eval epoch-7, step-536 ---
2023-12-21 14:16:13 kl_divergence: 0.2055
2023-12-21 14:16:13 mse: 0.0000
2023-12-21 14:16:13 mae: 0.0000
2023-12-21 14:16:13 loss: 2360552.0833
2023-12-21 14:16:13 New best kl_divergence score (0.2055) at epoch-7, step-536
2023-12-21 14:16:13 
2023-12-21 14:16:43 --- Train epoch-8, step-603 ---
2023-12-21 14:16:43 loss: 2533359.4688
2023-12-21 14:16:47 --- Eval epoch-8, step-603 ---
2023-12-21 14:16:47 kl_divergence: 0.2095
2023-12-21 14:16:47 mse: 0.0000
2023-12-21 14:16:47 mae: 0.0000
2023-12-21 14:16:47 loss: 2370273.0278
2023-12-21 14:16:47 
2023-12-21 14:17:17 --- Train epoch-9, step-670 ---
2023-12-21 14:17:17 loss: 2532307.4701
2023-12-21 14:17:21 --- Eval epoch-9, step-670 ---
2023-12-21 14:17:21 kl_divergence: 0.2089
2023-12-21 14:17:21 mse: 0.0000
2023-12-21 14:17:21 mae: 0.0000
2023-12-21 14:17:21 loss: 2361183.9653
2023-12-21 14:17:21 
2023-12-21 14:17:52 --- Train epoch-10, step-737 ---
2023-12-21 14:17:52 loss: 2533572.8083
2023-12-21 14:17:56 --- Eval epoch-10, step-737 ---
2023-12-21 14:17:56 kl_divergence: 0.1970
2023-12-21 14:17:56 mse: 0.0000
2023-12-21 14:17:56 mae: 0.0000
2023-12-21 14:17:56 loss: 2360903.3403
2023-12-21 14:17:56 New best kl_divergence score (0.1970) at epoch-10, step-737
2023-12-21 14:17:56 
2023-12-21 14:18:27 --- Train epoch-11, step-804 ---
2023-12-21 14:18:27 loss: 2530745.7491
2023-12-21 14:18:31 --- Eval epoch-11, step-804 ---
2023-12-21 14:18:31 kl_divergence: 0.2119
2023-12-21 14:18:31 mse: 0.0000
2023-12-21 14:18:31 mae: 0.0000
2023-12-21 14:18:31 loss: 2363600.3750
2023-12-21 14:18:31 
2023-12-21 14:19:03 --- Train epoch-12, step-871 ---
2023-12-21 14:19:03 loss: 2530489.0667
2023-12-21 14:19:08 --- Eval epoch-12, step-871 ---
2023-12-21 14:19:08 kl_divergence: 0.2010
2023-12-21 14:19:08 mse: 0.0000
2023-12-21 14:19:08 mae: 0.0000
2023-12-21 14:19:08 loss: 2365807.5972
2023-12-21 14:19:08 
2023-12-21 14:19:38 --- Train epoch-13, step-938 ---
2023-12-21 14:19:38 loss: 2531198.8797
2023-12-21 14:19:42 --- Eval epoch-13, step-938 ---
2023-12-21 14:19:42 kl_divergence: 0.2035
2023-12-21 14:19:42 mse: 0.0000
2023-12-21 14:19:42 mae: 0.0000
2023-12-21 14:19:42 loss: 2360327.6667
2023-12-21 14:19:42 
2023-12-21 14:20:12 --- Train epoch-14, step-1005 ---
2023-12-21 14:20:12 loss: 2530045.4813
2023-12-21 14:20:17 --- Eval epoch-14, step-1005 ---
2023-12-21 14:20:17 kl_divergence: 0.1983
2023-12-21 14:20:17 mse: 0.0000
2023-12-21 14:20:17 mae: 0.0000
2023-12-21 14:20:17 loss: 2357692.1806
2023-12-21 14:20:17 
2023-12-21 14:20:49 --- Train epoch-15, step-1072 ---
2023-12-21 14:20:49 loss: 2530552.7066
2023-12-21 14:20:54 --- Eval epoch-15, step-1072 ---
2023-12-21 14:20:54 kl_divergence: 0.1977
2023-12-21 14:20:54 mse: 0.0000
2023-12-21 14:20:54 mae: 0.0000
2023-12-21 14:20:54 loss: 2364016.3542
2023-12-21 14:20:54 
2023-12-21 14:21:26 --- Train epoch-16, step-1139 ---
2023-12-21 14:21:26 loss: 2529993.7598
2023-12-21 14:21:30 --- Eval epoch-16, step-1139 ---
2023-12-21 14:21:30 kl_divergence: 0.2001
2023-12-21 14:21:30 mse: 0.0000
2023-12-21 14:21:30 mae: 0.0000
2023-12-21 14:21:30 loss: 2357103.1806
2023-12-21 14:21:30 
2023-12-21 14:22:00 --- Train epoch-17, step-1206 ---
2023-12-21 14:22:00 loss: 2528716.8046
2023-12-21 14:22:05 --- Eval epoch-17, step-1206 ---
2023-12-21 14:22:05 kl_divergence: 0.2148
2023-12-21 14:22:05 mse: 0.0000
2023-12-21 14:22:05 mae: 0.0000
2023-12-21 14:22:05 loss: 2363072.6389
2023-12-21 14:22:05 
2023-12-21 14:22:37 --- Train epoch-18, step-1273 ---
2023-12-21 14:22:37 loss: 2528853.2313
2023-12-21 14:22:41 --- Eval epoch-18, step-1273 ---
2023-12-21 14:22:41 kl_divergence: 0.2090
2023-12-21 14:22:41 mse: 0.0000
2023-12-21 14:22:41 mae: 0.0000
2023-12-21 14:22:41 loss: 2359890.1806
2023-12-21 14:22:41 
2023-12-21 14:23:12 --- Train epoch-19, step-1340 ---
2023-12-21 14:23:12 loss: 2530428.1087
2023-12-21 14:23:16 --- Eval epoch-19, step-1340 ---
2023-12-21 14:23:16 kl_divergence: 0.2061
2023-12-21 14:23:16 mse: 0.0000
2023-12-21 14:23:16 mae: 0.0000
2023-12-21 14:23:16 loss: 2358671.3264
2023-12-21 14:23:16 Loaded best model
